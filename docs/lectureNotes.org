
* Data Scientist Toolbox Week 3:

** Types of Data Questions
*** Descriptive
    Like census data - what is the data? no analysis
    Ngram viewer
    No generalizing
    No interpretation
*** Exploratory
    Find new relationships/connections
    Find correlations - not causations
    No the final say on a finding
    fMRI scans, night sky imaging
*** Inferential
    Small sample analysis to model bigger samples
    Analyze subset to INFER about bigger population
    How pollution in one city affects health
*** Predictive
    Measure a set X to predict Y
    NOT causation
    Prediction models
    Polling data to predict elections
*** Causal
    Randomized studies on variable X affecting Y
    Relationships are identified as averages
    The "gold standard"
*** Mechanistic
    Measure exact changes in variables
    Engineering, material sciences, etc.

** What's data!?
*** Data: values of qualitative or quantitative variables, belonging to a set of items
*** Variable: measurement of a characteristic of an item 
**** Qualitative: type, origin, sex, descriptive
**** Quantitative: numbers, blood pressure, height, etc.
*** API content, medical record, tables, JSON, images, audio, map, 
    Data.gov
    DarwinTunes
*** Is messy, not a table
*** _Data follows question_

** Big Data
*** Is there a _big question_ for it?
*** Cheap hardware = Storage is cheap
*** Internet = Collection is cheap
*** You need _right_ data, not _big_

** Experimental design
*** Will define if your data is right
*** Plan to share data and code
    figshare.com
    github.com
*** Questions before data
*** Confounding variables! Spurious correlation
    Chocolate consumption vs Noble prize per capita
    Shoe size vs Literacy
*** Prediction quantities
    Probability of positives/negatives
    False positives vs negatives
*** Data dredging - positive finding bias

* R Language Week 1:

** Reading Tables
*** https://stat.ethz.ch/R-manual/R-devel/library/utils/html/read.table.html
*** Defaults: space separated, '#' = comment, strings are factors
*** ~colClasses~ defines classes for each column => use it make building the table faster
*** Big Data considerations:
**** All read data is stored in memory -> use ~nrows~ to reduce memory use
**** 32 or 64 bit?
**** Other users on the machine?
**** types of data in column?
     numeric = 8 bytes
     * 1.5mil rows
     * 120 columns
     = 1.34GB
     + overhead
**** On average = expect to use twice as much RAM as your data takes on HDD

** ~dump()~ and ~dput()~ for Textual Formats
*** Getting data: ~source()~ and ~dget()~
*** metadata is included; corruption is easier to fix
*** ~dput()~ constructs R code based on R object input to read it into R later
*** ~dump()~ same, but can take multiple R objects
** Connections
*** opens files, gzfiles, bzfiles and url
*** can define type: reading, writing, etc.
*** useful for reading subsets
** [] vs [[]] vs $
*** [ returns same object type; can return MULTIPLE objects (so it will return a list/vector, not the actual values)
list[1] will return a list
*** [[ returns only a single object; may be of different type; can compute the index that is passed
list[[name]] or list[[variable]] or list[[functionOutput]]
*** $ returns object by NAME, otherwise the same as [[
list$name
*** ~matrix[]~ will ~drop~ 2nd dimension, so it will return a list, not matrix
*** ~list$nam~ can do partial matching: "nam" = "name"; same for [[]] if you set ~exact=FALSE~

** ~is.na()~ and ~is.nan()~ and ~complete.cases()~  
*** logical output; checks for missing values 
*** NA is a builtin type
*** NaN is Not A Number != NA
** Vectorized coputations 
*** built-in looping for vector: computing/comparing values by index for 2+ vectors/matricies
vector(x) + vector(y) = vector(x1+y1, x2+y2, etc)
*** True/dot products are: %*% or %+% 

* Cleaning Data Week 1

** Raw vs Processed 
*** Raw data is just collected - it might look terrible - it was not altered
*** Binary file, unformatted excel, JSON, handwritten
*** Quantitative vs Qualitative data
*** Preprocessing: merging, subsetting, transforming
*** *ALWAYS RECORD ALL THE PROCESSING YOU DO ON DATA*
*** Record the TIME and DATE when you got the data
*** Processed data is ready for analysis
** Tidy data
*** After processing: still have raw data + tidy data + code book (aka metadata)
*** *Must report all steps from raw data to tidy data*
*** Third Normal Form - each variable has only one column, each observation gets one row
*** One table for each kind of variable; tables can be linked by a column; one file per table;
*** Have a metadata header; human readable names;
** Code Book: 
*** units, summary choices, 
*** type of study done; a markdown/doc file, 
*** study design description
** Instruction list:
*** a script that was used to process data;
*** takes no parameters!
*** If no script possible: include exact steps done
*** Be *extremely* explicit with all the details
** Reading data
*** read.table() - most common, loads data in RAM!,
*** can be slow, reads flat files
*** you can set na.strings, skip, head, headers, remove quotation marks
*** Excel files - they suck, but they're still popular
** _EXCEL_ - read.xlsx() - tell which sheet to read, and if there's a header
*** can read specific rows and columns
*** you can write.xlsx(); or try XLConnect;
** _XML_ - mostly on the web; extensible markup language 
*** Markup - start/end/empty tags <tag></tag>
*** Some tags have attributes <a src="okok"/>
*** xmlTreeParse() - makes it into a structured object
*** xmlRoot() - gets the main tree; rootNode[[]] - can be subsetted
*** xmlNames(node) - spits out tags within the node; xmlValue(node) - gets values from between tags
*** xmlSApply(node,"XPATH", function) - apply a function programmatically to a node
*** XPath - language for processing XML
/node - top level node
//node - any level node
node[@attr-name] - node with an attribute name
node[@attr-name='foo'] - node with attribute name set to 'foo'
** _JSON_ - close to XML, used a lot in APIs, different syntax and supports many value types
*** fromJSON(url) - makes a data.frame; you can have data.frames within data.frames
*** toJSON() - to make a JSON
** _data.table_ - package that improves on data.frame; much faster
*** tables() - lists all tables in memory
*** subsetting columns is different, but you can pass variables to add/summarize/change columns
*** .N, by=varName - .N will hold how many a certain group of varName appears
*** setkey() did not have a clear explanation
*** data.table reads files faster than data.frame

* Cleanind Data Week 2

** Reading mySQL
*** linked tables; kind of like linked data.frames;
*** https://www.pantz.org/software/mysql/mysqlcommands.html
*** requires installation; uses RMySQL package; uses query language
*** use fetch() to get smaller amounts
*** remember to _clear_ the query and _close_ the connection
** Reading HDF5
*** hierarchical data format
*** groups and datasets; metadata
*** some complicated stuff;
*** https://www.bioconductor.org/packages/release/bioc/html/rhdf5.html
** Scraping the web
*** some websites don't like it;
*** readLines from a connection; or use XML package; or httr package; 
*** you get authenticate; save handles;
*** https://www.r-bloggers.com/search/web%20scraping/
** Getting data from APIs
*** sometimes requires accounts to get API ID
*** user httr package; and the website's documentation
** Other useful packages
*** GOOGLE IT
*** file; url; gzfile; bzfile; connections;
*** foreign pacakge - for other programming languages
*** RPostresSQL; RODBC; RMongo;  
*** Images - jpeg; readbitmap; png; EBImage;
*** GIS - rdgal; rgeos; raster; 
*** Music - tuneR; seewave;
* Exploratory Analysis
** Principles
by Edward Tufte
*** Show comparisons
At least to a null hypothesis
*** Show correlation, mechanism, explanation
Control variable plots
*** Show multivariate data
Not just two variables
*** Multiple modes of evidence
Not just tables/plots
*** Document the evidence
R Code
Readme
How did you get to here?
*** Content is king
Have a story
Does all this have a point?
** Exploratory graphs
*** Make these for yourself - to see what's up
Debug analysis
Find patterns
Suggest modeling
*** Make LOTS of them
Don't worry about presentation
** One dimensional summaries
*** Five number summary: mean, median, min, max, quartiles, etc.
*** Boxplot
Broken up by quartiles
*** Histogram
*** Density plot
*** Barplot
** Multidim Plots
http://www.r-graph-gallery.com/
*** Overlayed plots; coplots
*** Add colors, shapes, sizes!
*** Spinning plots
*** Actual 3D plots
** Plotting systems
*** What is the target medium? File/Screen/Print?
*** How much data to use? Resizable?
*** Can't mix the systems
*** Base plotting system
Make a simple plot with a base function
Then add color using annotation functions
Can't 'remove' elements; no graphical language;
**** Packages: ~graphics~, ~grDevices~
**** For screens mainly
**** Make plot with: ~plot()~, ~hist()~, ~with()~, ~boxplot()~
pch - plotting symbol
lty - line type
lwd - line width
col - plotting color
xlab - label for x-axis
ylab - label for y-axis
**** Define global parameters with: ~par()~
las - orientation of the axis labels
bg - background color
mar - margin size
oma - outer margin size for whole plot; outer title can go here
mfrow - number of plots per row+column - plots fill row-wise
mfcol - number of plots per row+column - plots fill column-wise
**** Call ~par("arg")~ to see current value
**** Other funcs:
~abline~, ~lines~ add lines to plot
~points~ add points
~text~, ~title~, ~mtext~ to add text/titles
~axis~ axis labels
*** Lattice system
'lattice' package
**** One function for the whole load of plots at once
It can be hard to put together all parameters at once; can't reannotate
**** Packages: ~lattice~, ~grid~
**** Formula: ~(y ~ x | f * g), data~
plot _y_ over _x_ for every level of _f_ and _g_
y over x as axis
f and g are categorical (and optional)
data is where the numbers are
**** Easy to make a bunch of plots at once
**** Lattice functions return ~trellis~
system auto-prints it, but you can save it to an object
**** Panel functions to add extra lines to plots:
panel = func(x,y, ...){
panel.xyplot(x,y, ...) # calls default
panel.abline(h=median(x,y), col=3 ) # whatever line you wanna add
panel.lmline(x,y, col=2) }
**** Can't use base plotting parameters
**** Default spacing/margins are usually fine
*** The infamous ~ggplot2~
Grammar of graphics by Wilkinson
Improvement over lattice; customizable defaults
**** Grammar theory: verb, noun, adj, etc.
**** ~qplot()~
***** Quick plot like base ~plot()~
***** use data.frames
***** _aesthetics_ - size, shape, color
***** _geoms_ - points, lines
***** label your factors!
***** _facets_ for multiplots
***** _scales_ how to split data (color)
***** _stats_ smoothing, quantiles, etc.
**** ~ggplot()~
***** layer plots like base system
data then aesthetics
overlay a summary
add metadata
***** initial call will not make a plot
***** ~ggplot() + geom_point()~ adds points
~geom_smooth()~ adds line
***** now it can print a plot
***** ~face_grid(row ~ col)~ splits into plots by category
use factors for this
***** ~labs()~, ~ggtitle()~, ~theme()~ - annotation options
***** aesthetics can be varied by variable with ~aes()~
***** 
**** Is pretty, but not too customizeable outside of presets
**** ggplot will remove data points outside of plot limits
**** try ~cut()~ to split data by categories/factors
*** Graphics Devices
~?Devices~
**** Screen, PDF, bitmap, SVG
x11() for linux
**** You can open/close file graphics device
Instead of screen, plot is generated into file
**** Vector vs Bitmap files
Vectors resize well
Vector are not good for lots of points - big filesize
SVG can do animations
Bitmaps do not resize well 
Bitmaps are good for color
**** You can open multiple devices at the same time
**** But only plot to one device at a time
**** ~dev.cur()~ and ~dev.set()~ for multiple devices
**** ~dev.copy()~ copy plot to another device (MIGHT DISTORT)
** Clustering analysis
*** Hierarchical clustering
**** agglomerative:
**** _process_: closest things together, find next closest
**** _needs_: be able to calculate distance, ability to merge
**** _result_: tree that defines closeness
**** _problems_: What does distance mean?
euclidean - literal difference, continuous
manhattan - node by node, no direct comparison 
**** issues: unstable to changes, defining distance, defining scaling
**** dendogram - plot of clusters
**** merging: what is the distance of cluster?
average of all distances?
or largest distance?
**** ~heatmap()~ - reorders table based on clustering 
*** K-means clustering
**** define distance
**** what is a group? - predefined amount of clusters
**** _centroid_ - center of cluster
**** data is assigned to centroid
**** recalculate centroids and clusters until they are close
**** is not deterministic - results may vary
*** Dimension Reduction
**** reduce amount of variables - uncorrelated variables that explain the rest of the variables
some variables depend on others - weight to height, we we can reduce it to one variable
**** Singular Value Decomposition
U - orthogonal left singular vectors
D - orthogonal rigth singular vectors
V^t - diagonal matrix with singular values
**** Principal Components Analysis
subtract column means and divide by stdev
then run SVD to get stuff
**** Explains % of variation based on specified component
**** Doesn't work with NAs
**** ~impute~ calculates NA based on nearest neighbors
**** Lots of computation; scale-dependent
* Colors!
** ~grDevices~
*** ~colorRamp~ takes 2 colors and can return a blend of 2 based on distance (0 to 1)
*** ~colorRampPalette~ return a vector of colors between the 2 colors given
** ~RColorBrewer~
*** Sequential palette - ordered gradients
*** Diverging palette - diverging gradients
*** Qualitative palette - not ordered data
** generate colors and pass them to ~col=~ parameter
** ~smoothScatter~ to make overlapping prettier
** ~rgb~ + ~alpha=~ to adjust color and opacity
great to see density/clustering
** ~colorspace~ is also cool
** Colors communicate well :D
* Reproducible Research
** Create notation for reasearch
** Steps of data analysis
** Define the question
Make sure you have an interesting question
If you just apply tools at random - 
you will find something interesting
but it might not be important
** Define the ideal data set
Too much data
Descriptive, exploratory, inferential, predictive, casual, mechanistic
different data for different goals
** Determine what data set you can actually get
You never get ideal data
sampling can mess you up
MAKE THE DATA
*** Obtain data
Always reference the source
Get access time
*** Clean data
Was it pre-processed?
Subset, reformat, etc.
DOCUMENT YO SELF
Sometimes you have to go back to find a better dataset
*** Exploratory data analysis
Figure out the structure of data
Plots, summaries
Clustering, etc.
*** Statistical prediction/modeling
Document your transformations!
*** Interpret results
Watch your language
Explain the results, and why they are off
*** Challenge results
Cause someone else sure will
Where can this be done better?
*** Syntehsize results
The story of your analysis
Don't include everything, but keep it handy
The story doesn't follow your timeline
Make it pretty
*** Create reproducible code
So that someone else can do it
** Organizing analysis
*** Data
Raw or not
*** Figures
Exploratory
Final
*** R code
Raw scripts
Modeling scripts
*** Text
Annotations
Readmes, etc
Report
** RPubs
*** Free account to publish your research
*** Can use ~knitr~ and RStudio to publish
** Project Checklist
*** Start with Good Science
**** Coherent question
**** Tidy data
*** Don't do things by hand
**** It's too hard to document those things
**** Don't clean/download/move data
**** Don't do "just this one time"
*** Don't use GUI tools
**** Same as "doing by hand"
**** Interactive software means higher complexity, means lower stability
*** Make an algorithm
**** Teach a computer to do your analysis
**** Requires precise instructions
*** User Version Control
**** Slows you down - keep track of details
**** Keeps a log of what you've done
**** Allows to backtrack
*** Keep track of software environment
**** Computer specs
**** Langages, libraries, pacakges, dependencies, versioning
**** ~sessionInfo()~
*** Don't save output
**** Not until the very end
**** Save the code instead
*** Set your seeds
**** Random number generators are not reproducible
**** If you set seed, at least it's something
*** Manage the pipeline
**** Keep track of the process
**** Have milestones
** The Pipeline
*** Question
*** Protocol
*** Measured Data -> Processing Code
*** Analytic Data -> Analytic Code
*** Computational Results -> Presentation Code
*** Figres/Tables/Numerical Summaries
*** Published Article
** Reproducibility
*** Not replication - most things cannot be replicated
*** Does not guarantee validity
*** Not everyone is on the same page - some might disagree
*** Fixes problems only post-research, not before/during
*** Still, have to try alternative approaches
*** Who's going to bother to reproduce?
** Deterministic Statistical Machine
*** Standardized pipeline
*** Transparent box
*** Reduce "degrees of freedom"
*** Like clinica trial protocol
*** Different research will require different pipelines
** ~Cacher~ package
*** Storing intermediate results
*** Saves code and data
*** *OK NEVERMIND IT IS DISCONTINUED*
* Statistical Inference
** Frequentist paradigm
*** vs Bayesian
*** % of time something happens
** Probability models
*** Sum of randomness - potential outcomes
*** Probability is 0 to 1
*** Exclusive sets of outcomes add up to 1
*** P(Nothing) = 0
*** P(Something) = 1
*** P(X) = 1 - P(-X)
*** If A implies B then P(A) is less than P(B)
A is a subset of B
*** P(A&B) is sum of P(A) plus P(B) minus intersection
If you add A and B then you get the overlap twice
** Random Variable and its Functions
*** Discrete = countable, quantifiable outcomes (hair color, number of hits, etc)
*** Continuous = on a continuum (assigned by ranges, BMI of subject, IQ for a population)
*** *Probability Mass Function* = assigns a Pr to a variable outcome
1/6 for a die roll of 1
*** PMF is always > 0
*** Sum of all PMF is 1
*** *Probability Densify Function* = for continuous variables
*** PDF > 0 everywhere
*** Total area under PDF = 1
*** *Cumulative Distribution Function* = probability of result less than X
*** *Survival function* = reverse of CDF
*** Quantiles = PDF of Xth percentile
Probability of random X being below the Quantile is equal to the Quantile
** Conditional Probability
*** Condition selects a sample from all probabilities
*** P(A|B) = P(A&B) / P(B)
Pr of B given that A has occured = Pr of intersection divided by Pr of B
** Bayes' Rule
*** P(B|A) = (P(A|B)P(B))/(P(A|B)P(B) + P(A|-B)P(-B))
*** Diagnostic test - get Sensitivity/Specificity and Predictive values
*** Test positive knowing that Disease is present
*** Likelihood ratio -> Sensitivity divided by False positive
** Independence
*** Only independent probabilities can be multiplied
*** Independent and Identically Distributed variables = default model for random samples
** Expected Values
*** Mean is the center of the distribution - like center of mass
*** Population mean = Sum of values of X times Pr of X
For a die:
(1+2+3+4+5+6)*(1/6) = 3.5
*** Variance is the spread of the distribution
*** Sample mean is center of mass if Pr of all X was equal
*** For continuous variable, mean is the 1/2 area
*** Mean of sample means is estimation of population mean - estimator
*** In other words: distribution of averages will center on the center of the population
Population mean of coin flips is 0.5
Average of 10 coin flips is around 0.5 - sample mean
Average of 10 averages of 10 coin flips is closer to 0.5 - mean of sample mean
** Variance
*** Var(X) = E[(X-u)^2] = E[X^2] - E[X]^2
*** sqrt(Var) = standard deviation
*** Variance - how spread the function is from the mean
*** Sample variance - avg(observed variance) (divide by n-1)
*** Sample variances have a distribution - it centers around population variance
*** Dividing by (n-1) is what makes it unbiased???
*** Var(sameple mean) = (mu^2)/n -> closer to 0 with more sample data
*** Standard error - stdev/(sqrt(n)) - how much sample means deviate from population mean
** Distributions
*** Bernoulli - binary outcome
**** P(x) = p^x(1-p)^(1-x)
**** mean = p
**** Var = p(1-p)
*** Binomial - sum of Bernoulli variables
**** Same as Bernoulli but with (n choose x) upfront
*** Normal/Gaussian - bell curve 
**** mean = mu
**** Var = sig^2
**** standard normal - mu = 0, sig = 1
**** Standard deviation is good to use to estimate probabilities
*** Poisson - lambda^x(e^-lambda)/fact(x)
**** Probability of x events happening when the average per interval is lambda
**** mean = lambda
**** Var = lambda
**** Used to model count data, small p binomials, contingency tables, rates, etc.
**** When n is big and p is small it's close to Binomial
** Asymptotics
*** Behavior of statistic as sample size goes to Inf
*** *Law of Large Numbers* - average converges to what it is estimating
*** Estimator is _consistent_ if it converges to estimate
*** Sample mean, variance and deviation are _consistent_ 
*** *Central Limit Theorem* - distribution of means of iid variables converges to standard normal distribution
*** (Estimate - Mean of Estimate)/(StdErr of Estimate) -> standard normal distribution for large n
*** *Confidence interval* - interval estimate of a population parameter;
90% confidence interval = probability that the real value lies within the interval of the sample
*** *Confidence level* - with more confidence intervals -> proportion of intervals that will contain the true value of the parameter
*** Mean +/- normal quantile time StdErr yields confidence interval for the mean
* Confidence Intervals
** T Confidence Intervals
*** Est +/- TQ*SEest
*** Smaller samples
*** T interval for quantile
*** Degrees of Freedom - smaller samples generate narrow confidence intervals
*** Xbar +/- T(n-1)S/sqrt(n)
*** T(n-1) = quantile
*** Heavier tails than normal dist
*** T interval assumes data is kinda normal dist
*** Don't use it for skewed dist
*** For independent groups we take diff of averages
Ybar-Xbar +/- t(nt+ny-2)*Sderr(1/nx+1/ny)^.5
*** If sample sizes are different, need to weight variance differently
*** Unequal variance interval - crazy formula for degrees of freedom
** Hypothesis testing
*** Null hypothesis = status quo = H0
*** In order to make inference - null needs to be disproved
*** Ha = hypothesis to test
*** Type I error = decide Ha when H0 is true
*** Type II error = decide H0 when Ha is true
*** Pr of Type I rate ~ 5%
*** StdErr of Mean = Stdev/sqrt(sample size)
*** Find a 95% confidence interval for distribution of the mean = Type I err = 5%
*** Two sided test require wider intervals 5% -> 2.5% on each side
*** Two-group test = compare means of two groups
** P-values
*** Compare Type I error rate to P value
*** If P value < then reject N0
*** P-value = probability of getting data > extreme than observed data
*** How likely that our samples will have statistic this large if N0 is true?
*** If not likely - then N0 is rejected
*** Assuming N0 - how unusual is our observation?
*** P-value - opposide of confidence interval - alpha level for the hypothesis test
*** If P < alpha level = reject N0
*** 
* Power
** Probability of rejecting H0 when it's false - good thing
** Type II Error - failing to reject H0 when it's false - error rate = beta
** Power = 1 -  beta
** Power depends on the mean under H0 - for detecting deviations
** Power curve - more likely to detect bigger difference
** With more data - higher chance of detecting deviations
** Power is Pr of Mean being bigger than Mean under H0 + standard error quantiles
** Unknowns: MeanA, sigma, n and beta
** Knowns: Mean0, alpha
** Effect size: difference of means divided by sigma - for bigger power
(sqrt(n)(mu-mu0))/sigma
** ~power.t.test~ does everything for you...
* Multiple testing
** Multiple testing is correcting for false positives
** 5% error chance is still a possibility - after 20 tests something will show up
** False error rate - beta = 0
** Family wise error rate - Pr of at least one false positive
** False discovery rate - false significance
** P-values control error rate at alpha - can we do better?
** Bonferroni correction - for family wise error rate - set alpha to be alpha/(number of tests)
** Very conservative
** False discovery rate - calculate p-values and accept ith p-value smaller than alpha*(i/m)
* Resampling
** Bootstrap - sample from empirical data, not from total population
** Sample with replacement values - resample a lot of times and make an estimate from that
resample n observations each time
** With replacement -> Each observation has Pr 1/n -> some observations will be repeated
** Use the resampled data to estimate statistics for total population
** Monte Carlo error?
** Efron and Tibshirani on bootstrap
* Permutation tests
** If H0 is that a statistic is the same for 2 groups
** Mix the data and permute it and recalculate the statistic
** Calculate where permuted data generated an extreme statistic = p-value
