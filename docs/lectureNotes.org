
* Data Scientist Toolbox Week 3:

** Types of Data Questions
*** Descriptive
    Like census data - what is the data? no analysis
    Ngram viewer
    No generalizing
    No interpretation
*** Exploratory
    Find new relationships/connections
    Find correlations - not causations
    No the final say on a finding
    fMRI scans, night sky imaging
*** Inferential
    Small sample analysis to model bigger samples
    Analyze subset to INFER about bigger population
    How pollution in one city affects health
*** Predictive
    Measure a set X to predict Y
    NOT causation
    Prediction models
    Polling data to predict elections
*** Causal
    Randomized studies on variable X affecting Y
    Relationships are identified as averages
    The "gold standard"
*** Mechanistic
    Measure exact changes in variables
    Engineering, material sciences, etc.

** What's data!?
*** Data: values of qualitative or quantitative variables, belonging to a set of items
*** Variable: measurement of a characteristic of an item 
**** Qualitative: type, origin, sex, descriptive
**** Quantitative: numbers, blood pressure, height, etc.
*** API content, medical record, tables, JSON, images, audio, map, 
    Data.gov
    DarwinTunes
*** Is messy, not a table
*** _Data follows question_

** Big Data
*** Is there a _big question_ for it?
*** Cheap hardware = Storage is cheap
*** Internet = Collection is cheap
*** You need _right_ data, not _big_

** Experimental design
*** Will define if your data is right
*** Plan to share data and code
    figshare.com
    github.com
*** Questions before data
*** Confounding variables! Spurious correlation
    Chocolate consumption vs Noble prize per capita
    Shoe size vs Literacy
*** Prediction quantities
    Probability of positives/negatives
    False positives vs negatives
*** Data dredging - positive finding bias

* R Language Week 1:

** Reading Tables
*** https://stat.ethz.ch/R-manual/R-devel/library/utils/html/read.table.html
*** Defaults: space separated, '#' = comment, strings are factors
*** ~colClasses~ defines classes for each column => use it make building the table faster
*** Big Data considerations:
**** All read data is stored in memory -> use ~nrows~ to reduce memory use
**** 32 or 64 bit?
**** Other users on the machine?
**** types of data in column?
     numeric = 8 bytes
     * 1.5mil rows
     * 120 columns
     = 1.34GB
     + overhead
**** On average = expect to use twice as much RAM as your data takes on HDD

** ~dump()~ and ~dput()~ for Textual Formats
*** Getting data: ~source()~ and ~dget()~
*** metadata is included; corruption is easier to fix
*** ~dput()~ constructs R code based on R object input to read it into R later
*** ~dump()~ same, but can take multiple R objects
** Connections
*** opens files, gzfiles, bzfiles and url
*** can define type: reading, writing, etc.
*** useful for reading subsets
** [] vs [[]] vs $
*** [ returns same object type; can return MULTIPLE objects (so it will return a list/vector, not the actual values)
list[1] will return a list
*** [[ returns only a single object; may be of different type; can compute the index that is passed
list[[name]] or list[[variable]] or list[[functionOutput]]
*** $ returns object by NAME, otherwise the same as [[
list$name
*** ~matrix[]~ will ~drop~ 2nd dimension, so it will return a list, not matrix
*** ~list$nam~ can do partial matching: "nam" = "name"; same for [[]] if you set ~exact=FALSE~

** ~is.na()~ and ~is.nan()~ and ~complete.cases()~  
*** logical output; checks for missing values 
*** NA is a builtin type
*** NaN is Not A Number != NA
** Vectorized coputations 
*** built-in looping for vector: computing/comparing values by index for 2+ vectors/matricies
vector(x) + vector(y) = vector(x1+y1, x2+y2, etc)
*** True/dot products are: %*% or %+% 

* Cleaning Data Week 1

** Raw vs Processed 
*** Raw data is just collected - it might look terrible - it was not altered
*** Binary file, unformatted excel, JSON, handwritten
*** Quantitative vs Qualitative data
*** Preprocessing: merging, subsetting, transforming
*** *ALWAYS RECORD ALL THE PROCESSING YOU DO ON DATA*
*** Record the TIME and DATE when you got the data
*** Processed data is ready for analysis
** Tidy data
*** After processing: still have raw data + tidy data + code book (aka metadata)
*** *Must report all steps from raw data to tidy data*
*** Third Normal Form - each variable has only one column, each observation gets one row
*** One table for each kind of variable; tables can be linked by a column; one file per table;
*** Have a metadata header; human readable names;
** Code Book: 
*** units, summary choices, 
*** type of study done; a markdown/doc file, 
*** study design description
** Instruction list:
*** a script that was used to process data;
*** takes no parameters!
*** If no script possible: include exact steps done
*** Be *extremely* explicit with all the details
** Reading data
*** read.table() - most common, loads data in RAM!,
*** can be slow, reads flat files
*** you can set na.strings, skip, head, headers, remove quotation marks
*** Excel files - they suck, but they're still popular
** _EXCEL_ - read.xlsx() - tell which sheet to read, and if there's a header
*** can read specific rows and columns
*** you can write.xlsx(); or try XLConnect;
** _XML_ - mostly on the web; extensible markup language 
*** Markup - start/end/empty tags <tag></tag>
*** Some tags have attributes <a src="okok"/>
*** xmlTreeParse() - makes it into a structured object
*** xmlRoot() - gets the main tree; rootNode[[]] - can be subsetted
*** xmlNames(node) - spits out tags within the node; xmlValue(node) - gets values from between tags
*** xmlSApply(node,"XPATH", function) - apply a function programmatically to a node
*** XPath - language for processing XML
/node - top level node
//node - any level node
node[@attr-name] - node with an attribute name
node[@attr-name='foo'] - node with attribute name set to 'foo'
** _JSON_ - close to XML, used a lot in APIs, different syntax and supports many value types
*** fromJSON(url) - makes a data.frame; you can have data.frames within data.frames
*** toJSON() - to make a JSON
** _data.table_ - package that improves on data.frame; much faster
*** tables() - lists all tables in memory
*** subsetting columns is different, but you can pass variables to add/summarize/change columns
*** .N, by=varName - .N will hold how many a certain group of varName appears
*** setkey() did not have a clear explanation
*** data.table reads files faster than data.frame

* Cleanind Data Week 2

** Reading mySQL
*** linked tables; kind of like linked data.frames;
*** https://www.pantz.org/software/mysql/mysqlcommands.html
*** requires installation; uses RMySQL package; uses query language
*** use fetch() to get smaller amounts
*** remember to _clear_ the query and _close_ the connection
** Reading HDF5
*** hierarchical data format
*** groups and datasets; metadata
*** some complicated stuff;
*** https://www.bioconductor.org/packages/release/bioc/html/rhdf5.html
** Scraping the web
*** some websites don't like it;
*** readLines from a connection; or use XML package; or httr package; 
*** you get authenticate; save handles;
*** https://www.r-bloggers.com/search/web%20scraping/
** Getting data from APIs
*** sometimes requires accounts to get API ID
*** user httr package; and the website's documentation
** Other useful packages
*** GOOGLE IT
*** file; url; gzfile; bzfile; connections;
*** foreign pacakge - for other programming languages
*** RPostresSQL; RODBC; RMongo;  
*** Images - jpeg; readbitmap; png; EBImage;
*** GIS - rdgal; rgeos; raster; 
*** Music - tuneR; seewave;
* Exploratory Analysis
** Principles
by Edward Tufte
*** Show comparisons
At least to a null hypothesis
*** Show correlation, mechanism, explanation
Control variable plots
*** Show multivariate data
Not just two variables
*** Multiple modes of evidence
Not just tables/plots
*** Document the evidence
R Code
Readme
How did you get to here?
*** Content is king
Have a story
Does all this have a point?
** Exploratory graphs
*** Make these for yourself - to see what's up
Debug analysis
Find patterns
Suggest modeling
*** Make LOTS of them
Don't worry about presentation
** One dimensional summaries
*** Five number summary: mean, median, min, max, quartiles, etc.
*** Boxplot
Broken up by quartiles
*** Histogram
*** Density plot
*** Barplot
** Multidim Plots
http://www.r-graph-gallery.com/
*** Overlayed plots; coplots
*** Add colors, shapes, sizes!
*** Spinning plots
*** Actual 3D plots
** Plotting systems
*** What is the target medium? File/Screen/Print?
*** How much data to use? Resizable?
*** Can't mix the systems
*** Base plotting system
Make a simple plot with a base function
Then add color using annotation functions
Can't 'remove' elements; no graphical language;
**** Packages: ~graphics~, ~grDevices~
**** For screens mainly
**** Make plot with: ~plot()~, ~hist()~, ~with()~, ~boxplot()~
pch - plotting symbol
lty - line type
lwd - line width
col - plotting color
xlab - label for x-axis
ylab - label for y-axis
**** Define global parameters with: ~par()~
las - orientation of the axis labels
bg - background color
mar - margin size
oma - outer margin size for whole plot; outer title can go here
mfrow - number of plots per row+column - plots fill row-wise
mfcol - number of plots per row+column - plots fill column-wise
**** Call ~par("arg")~ to see current value
**** Other funcs:
~abline~, ~lines~ add lines to plot
~points~ add points
~text~, ~title~, ~mtext~ to add text/titles
~axis~ axis labels
*** Lattice system
'lattice' package
**** One function for the whole load of plots at once
It can be hard to put together all parameters at once; can't reannotate
**** Packages: ~lattice~, ~grid~
**** Formula: ~(y ~ x | f * g), data~
plot _y_ over _x_ for every level of _f_ and _g_
y over x as axis
f and g are categorical (and optional)
data is where the numbers are
**** Easy to make a bunch of plots at once
**** Lattice functions return ~trellis~
system auto-prints it, but you can save it to an object
**** Panel functions to add extra lines to plots:
panel = func(x,y, ...){
panel.xyplot(x,y, ...) # calls default
panel.abline(h=median(x,y), col=3 ) # whatever line you wanna add
panel.lmline(x,y, col=2) }
**** Can't use base plotting parameters
**** Default spacing/margins are usually fine
*** The infamous ~ggplot2~
Grammar of graphics by Wilkinson
Improvement over lattice; customizable defaults
**** Grammar theory: verb, noun, adj, etc.
**** ~qplot()~
***** Quick plot like base ~plot()~
***** use data.frames
***** _aesthetics_ - size, shape, color
***** _geoms_ - points, lines
***** label your factors!
***** _facets_ for multiplots
***** _scales_ how to split data (color)
***** _stats_ smoothing, quantiles, etc.
**** ~ggplot()~
***** layer plots like base system
data then aesthetics
overlay a summary
add metadata
***** initial call will not make a plot
***** ~ggplot() + geom_point()~ adds points
~geom_smooth()~ adds line
***** now it can print a plot
***** ~face_grid(row ~ col)~ splits into plots by category
use factors for this
***** ~labs()~, ~ggtitle()~, ~theme()~ - annotation options
***** aesthetics can be varied by variable with ~aes()~
***** 
**** Is pretty, but not too customizeable outside of presets
**** ggplot will remove data points outside of plot limits
**** try ~cut()~ to split data by categories/factors
*** Graphics Devices
~?Devices~
**** Screen, PDF, bitmap, SVG
x11() for linux
**** You can open/close file graphics device
Instead of screen, plot is generated into file
**** Vector vs Bitmap files
Vectors resize well
Vector are not good for lots of points - big filesize
SVG can do animations
Bitmaps do not resize well 
Bitmaps are good for color
**** You can open multiple devices at the same time
**** But only plot to one device at a time
**** ~dev.cur()~ and ~dev.set()~ for multiple devices
**** ~dev.copy()~ copy plot to another device (MIGHT DISTORT)
** Clustering analysis
*** Hierarchical clustering
**** agglomerative:
**** _process_: closest things together, find next closest
**** _needs_: be able to calculate distance, ability to merge
**** _result_: tree that defines closeness
**** _problems_: What does distance mean?
euclidean - literal difference, continuous
manhattan - node by node, no direct comparison 
**** issues: unstable to changes, defining distance, defining scaling
**** dendogram - plot of clusters
**** merging: what is the distance of cluster?
average of all distances?
or largest distance?
**** ~heatmap()~ - reorders table based on clustering 
*** K-means clustering
**** define distance
**** what is a group? - predefined amount of clusters
**** _centroid_ - center of cluster
**** data is assigned to centroid
**** recalculate centroids and clusters until they are close
**** is not deterministic - results may vary
*** Dimension Reduction
**** reduce amount of variables - uncorrelated variables that explain the rest of the variables
some variables depend on others - weight to height, we we can reduce it to one variable
**** Singular Value Decomposition
U - orthogonal left singular vectors
D - orthogonal rigth singular vectors
V^t - diagonal matrix with singular values
**** Principal Components Analysis
subtract column means and divide by stdev
then run SVD to get stuff
**** Explains % of variation based on specified component
**** Doesn't work with NAs
**** ~impute~ calculates NA based on nearest neighbors
**** Lots of computation; scale-dependent
* Colors!
** ~grDevices~
*** ~colorRamp~ takes 2 colors and can return a blend of 2 based on distance (0 to 1)
*** ~colorRampPalette~ return a vector of colors between the 2 colors given
** ~RColorBrewer~
*** Sequential palette - ordered gradients
*** Diverging palette - diverging gradients
*** Qualitative palette - not ordered data
** generate colors and pass them to ~col=~ parameter
** ~smoothScatter~ to make overlapping prettier
** ~rgb~ + ~alpha=~ to adjust color and opacity
great to see density/clustering
** ~colorspace~ is also cool
** Colors communicate well :D
